{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma =1.0\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it calculates the value function given a policy\n",
    "def policy_evaluation(policy):\n",
    "    \n",
    "    V = np.zeros(env.nS) # intialize V to 0's\n",
    "    \n",
    "    while True:\n",
    "        delta = 0 \n",
    "        \n",
    "        for s in range(env.nS):  # runs for 16 states  (0-15)\n",
    "            total_state_value = 0\n",
    "           \n",
    "            for a, prob_a in enumerate(policy[s]): #runs for 4 actions (0-3)\n",
    "        \n",
    "                for  prob_s, next_state, reward, _ in env.P[s][a]:\n",
    "                   total_state_value += prob_a * prob_s * (reward + gamma * V[next_state])\n",
    "            \n",
    "            delta = max(delta, np.abs(total_state_value - V[s])) # calculate change \n",
    "            V[s] = total_state_value\n",
    "            \n",
    "        # a check to terminate\n",
    "        if delta < 0.005:\n",
    "            break\n",
    "\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it improves the policy given a value function\n",
    "def policy_improvement(V,policy):\n",
    "    \n",
    "#     policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    for s in range(env.nS):\n",
    "        \n",
    "        Q_sa = np.zeros(env.nA)\n",
    "        \n",
    "        for a in range(env.nA):\n",
    "            for  prob_s, next_state, reward, _ in env.P[s][a]:\n",
    "                Q_sa[a] += prob_s * (reward + gamma * V[next_state])\n",
    "                \n",
    "        best_action = np.argmax(Q_sa)\n",
    "          \n",
    "        policy[s] = np.eye(env.nA)[best_action]\n",
    "            \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_iteration():\n",
    "    \n",
    "     # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    epochs = 1000\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        V = policy_evaluation(policy)\n",
    "        \n",
    "        old_policy = np.copy(policy)\n",
    "        \n",
    "        new_policy = policy_improvement(V,old_policy)\n",
    "        \n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "        \n",
    "    return policy,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 3.\n"
     ]
    }
   ],
   "source": [
    "final_policy, final_v = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Policy \n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Final Policy grid : (0=up, 1=right, 2=down, 3=left)\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "Final Value Function grid\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Policy \")\n",
    "print(final_policy)\n",
    "\n",
    "print(\"Final Policy grid : (0=up, 1=right, 2=down, 3=left)\")\n",
    "print(np.reshape(np.argmax(final_policy, axis=1), env.shape))\n",
    "\n",
    "print(\"Final Value Function grid\")\n",
    "print(final_v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimal_value_function(V):\n",
    "        \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "        \n",
    "            Q_sa = np.zeros(env.nA)\n",
    "        \n",
    "            for a in range(env.nA):\n",
    "                for  prob_s, next_state, reward, _ in env.P[s][a]:\n",
    "                    Q_sa[a] += prob_s * (reward + gamma * V[next_state])\n",
    "                    \n",
    "            max_value_function_s = np.max(Q_sa)\n",
    "            \n",
    "            delta = max(delta, np.abs(max_value_function_s - V[s]))\n",
    "            \n",
    "            V[s] = max_value_function_s\n",
    "        \n",
    "        if delta < 0.00001:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimal_policy_extraction(V):\n",
    "    \n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    for s in range(env.nS):\n",
    "        Q_sa = np.zeros(env.nA)\n",
    "\n",
    "        for a in range(env.nA):\n",
    "            for  prob_s, next_state, reward, _ in env.P[s][a]:\n",
    "                Q_sa[a] += prob_s * (reward + gamma * V[next_state])\n",
    "\n",
    "        best_action = np.argmax(Q_sa)\n",
    "\n",
    "        policy[s] = np.eye(env.nA)[best_action]\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration():\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    optimal_v = optimal_value_function(V)\n",
    "    \n",
    "    policy = optimal_policy_extraction(optimal_v)\n",
    "    \n",
    "    return policy,optimal_v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_policy,final_v = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Policy \n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Final Policy grid : (0=up, 1=right, 2=down, 3=left)\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "Final Value Function grid\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Policy \")\n",
    "print(final_policy)\n",
    "\n",
    "print(\"Final Policy grid : (0=up, 1=right, 2=down, 3=left)\")\n",
    "print(np.reshape(np.argmax(final_policy, axis=1), env.shape))\n",
    "\n",
    "print(\"Final Value Function grid\")\n",
    "print(final_v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal_policy_extraction and policy improvement functions are same\n",
    "\n",
    "but in policy iteration algorithm ( this has been looped ) , in value iteration algorithm it executes only one time."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
